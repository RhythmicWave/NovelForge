from typing import Awaitable, Callable, Optional, Type, Any, Dict, AsyncGenerator, Union
from fastapi import Response
from json_repair import repair_json
from pydantic import BaseModel, ValidationError
from pydantic_ai import Agent, ModelRetry
from pydantic_ai.models.openai import OpenAIChatModel
from pydantic_ai.providers.openai import OpenAIProvider
from pydantic_ai.models.anthropic import AnthropicModel
from pydantic_ai.providers.anthropic import AnthropicProvider
from pydantic_ai.models.google import GoogleModel
from pydantic_ai.providers.google import GoogleProvider
from pydantic_ai.settings import ModelSettings
from sqlmodel import Session
from app.services import llm_config_service
from loguru import logger
from app.schemas.ai import ContinuationRequest, AssistantChatRequest
from app.services import prompt_service
from app.db.models import LLMConfig
import asyncio
import json

# ä¸Šä¸‹æ–‡è£…é…
from app.services.context_service import assemble_context, ContextAssembleParams
# å¯¼å…¥éœ€è¦æ ¡éªŒçš„æ¨¡å‹
from app.schemas.wizard import StageLine, ChapterOutline, Chapter
import re


_TOKEN_REGEX = re.compile(
    r"""
    ([A-Za-z]+)               # è‹±æ–‡å•è¯ï¼ˆè¿ç»­å­—æ¯ç®— 1ï¼‰
    |([0-9])                 # 1ä¸ªæ•°å­—ç®— 1
    |([\u4E00-\u9FFF])       # å•ä¸ªä¸­æ–‡æ±‰å­—ç®— 1
    |(\S)                     # å…¶å®ƒéç©ºç™½ç¬¦å·/æ ‡ç‚¹ç®— 1
    """,
    re.VERBOSE,
)

def _estimate_tokens(text: str) -> int:
    """æŒ‰è§„åˆ™ä¼°ç®— tokenï¼š
    - 1 ä¸ªä¸­æ–‡ = 1
    - 1 ä¸ªè‹±æ–‡å•è¯ = 1
    - 1 ä¸ªæ•°å­— = 1
    - 1 ä¸ªç¬¦å· = 1
    ç©ºç™½ä¸è®¡ã€‚
    """
    if not text:
        return 0
    try:
        return sum(1 for _ in _TOKEN_REGEX.finditer(text))
    except Exception:
        # é€€åŒ–ï¼šæŒ‰éç©ºç™½å­—ç¬¦è®¡æ•°
        return sum(1 for ch in (text or "") if not ch.isspace())

from app.services import llm_config_service as _llm_svc

def _calc_input_tokens(system_prompt: Optional[str], user_prompt: Optional[str]) -> int:
    sys_part = system_prompt or ""
    usr_part = user_prompt or ""
    return _estimate_tokens(sys_part+usr_part) 


def _precheck_quota(session: Session, llm_config_id: int, input_tokens: int, need_calls: int = 1) -> None:
    ok, reason = _llm_svc.can_consume(session, llm_config_id, input_tokens, 0, need_calls)
    return ok,reason


def _record_usage(session: Session, llm_config_id: int, input_tokens: int, output_tokens: int, calls: int = 1, aborted: bool = False) -> None:
    try:
        _llm_svc.accumulate_usage(session, llm_config_id, max(0, input_tokens), max(0, output_tokens), max(0, calls), aborted=aborted)
    except Exception as stat_e:
        logger.warning(f"è®°å½• LLM ç»Ÿè®¡å¤±è´¥: {stat_e}")

def _get_agent(
    session: Session,
    llm_config_id: int,
    output_type: Optional[Type[BaseModel]] = None,
    system_prompt: str = '',
    temperature: float = 0.7,
    max_tokens: Optional[int] = None,
    timeout: float = 64,
    deps_type: Type = str,
    tools: list = None,
) -> Agent:
    """
    æ ¹æ®LLMé…ç½®å’ŒæœŸæœ›çš„è¾“å‡ºç±»å‹ï¼Œè·å–ä¸€ä¸ªé…ç½®å¥½çš„LLM Agentå®ä¾‹ã€‚
    ç»Ÿä¸€ä½¿ç”¨ ModelSettings è®¾ç½® temperature/max_tokens/timeoutï¼ˆæ— éœ€æŒ‰æä¾›å•†åˆ†æ”¯åˆ†åˆ«è®¾ç½®ï¼‰ã€‚
    
    æ–°å¢å‚æ•°ï¼š
    - deps_type: ä¾èµ–æ³¨å…¥ç±»å‹ï¼ˆé»˜è®¤ strï¼‰
    - tools: å·¥å…·åˆ—è¡¨ï¼ˆPydantic AI Tool å¯¹è±¡ï¼‰
    - output_type: è¾“å‡ºç±»å‹ï¼ˆNone è¡¨ç¤ºå…è®¸æ–‡æœ¬å’Œå·¥å…·è°ƒç”¨ï¼‰
    """
    llm_config = llm_config_service.get_llm_config(session, llm_config_id)
    if not llm_config:
        raise ValueError(f"LLMé…ç½®ä¸å­˜åœ¨ï¼ŒID: {llm_config_id}")

    if not llm_config.api_key:
        raise ValueError(f"æœªæ‰¾åˆ°LLMé…ç½® {llm_config.display_name or llm_config.model_name} çš„APIå¯†é’¥")
    print(f"=======llm_config.provider:{llm_config.provider}=========")
    # Provider & Model åˆ›å»ºï¼ˆä¸å†åœ¨æ­¤å¤„è®¾ç½®æ¸©åº¦/è¶…æ—¶ï¼‰
    if llm_config.provider == "openai":
        provider_config = {"api_key": llm_config.api_key}
        if llm_config.api_base:
            provider_config["base_url"] = llm_config.api_base
        provider = OpenAIProvider(**provider_config)
        model = OpenAIChatModel(llm_config.model_name, provider=provider)
    elif llm_config.provider == "anthropic":
        provider_config = {"api_key": llm_config.api_key}
        if llm_config.api_base:
            provider_config["base_url"] = llm_config.api_base
        provider = AnthropicProvider(**provider_config)
        model = AnthropicModel(llm_config.model_name, provider=provider)
    elif llm_config.provider == "google":

        provider = GoogleProvider(api_key=llm_config.api_key)
        model = GoogleModel(llm_config.model_name, provider=provider)
    elif llm_config.provider == "custom":
        provider_config = {"api_key": llm_config.api_key}
        if llm_config.api_base:
            provider_config["base_url"] = llm_config.api_base
        provider = OpenAIProvider(**provider_config)
        model = OpenAIChatModel(llm_config.model_name, provider=provider)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æä¾›å•†ç±»å‹: {llm_config.provider}")

    # ç»Ÿä¸€çš„æ¨¡å‹è®¾ç½®
    settings = ModelSettings(
        temperature=temperature,
        max_tokens=max_tokens,
        timeout=timeout,
        extra_body=None,
    )

    # åˆ›å»º Agentï¼ˆæ”¯æŒå·¥å…·å’Œè‡ªå®šä¹‰ä¾èµ–ç±»å‹ï¼‰
    if output_type is None:
        # ä¸æŒ‡å®š output_typeï¼Œé»˜è®¤å…è®¸æ–‡æœ¬è¾“å‡ºå’Œå·¥å…·è°ƒç”¨
        agent = Agent(
            model, 
            system_prompt=system_prompt, 
            model_settings=settings,
            deps_type=deps_type,
            tools=tools or []
        )
    else:
        # æŒ‡å®šäº† output_typeï¼Œä½¿ç”¨ Union[output_type, str] å…è®¸æ–‡æœ¬å›é€€
        agent = Agent(
            model, 
            system_prompt=system_prompt, 
            model_settings=settings,
            output_type=Union[output_type, str],
            deps_type=deps_type,
            tools=tools or []
        )
        agent.output_validator(create_validator(output_type))
    
    return agent

async def run_agent_with_streaming(agent: Agent, *args, **kwargs):
    """
    ä½¿ç”¨ agent.iter() å’Œ node.stream() è¿­ä»£è·å–æ¯ä¸ªæµå¼å“åº”å—å†…å®¹ï¼Œ
    ç„¶åè¿”å›æœ€ç»ˆçš„å®Œæ•´ç»“æœã€‚é¿å…ç›´æ¥è¿”å›ç»“æœæ—¶å‡ºç°ç½‘ç»œæ³¢åŠ¨å¯¼è‡´ç”Ÿæˆå¤±è´¥
    """
    async with agent.run_stream(*args, **kwargs) as stream:
        return await stream.get_output()


async def stream_agent_response(
    agent: Agent,
    user_prompt: str,
    *,
    deps=None,
    message_history: list = None,
    track_tool_calls: bool = True
) -> AsyncGenerator[str, None]:
    """
    é€šç”¨çš„æµå¼ Agent å“åº”ç”Ÿæˆå™¨ï¼Œæ”¯æŒå·¥å…·è°ƒç”¨å’Œæ–‡æœ¬æµå¼è¾“å‡ºã€‚
    
    **åŸºäº Pydantic AI å®˜æ–¹ç¤ºä¾‹å®ç°**ï¼š
    https://ai.pydantic.dev/agents/#iterating-over-an-agents-graph
    
    å·¥ä½œåŸç†ï¼š
    1. ä½¿ç”¨ agent.iter() è¿­ä»£æ¯ä¸ªèŠ‚ç‚¹ï¼ˆUserPrompt/ModelRequest/CallTools/Endï¼‰
    2. å¯¹äº ModelRequestNodeï¼šæ£€æµ‹ FinalResultEvent åæµå¼è¾“å‡ºæ–‡æœ¬
    3. å¯¹äº CallToolsNodeï¼šç›‘å¬ FunctionToolCallEvent å’Œ FunctionToolResultEvent
    4. å·¥å…·å·²åœ¨ Agent åˆ›å»ºæ—¶ç»‘å®šï¼Œä¼šè‡ªåŠ¨æ‰§è¡Œ
    5. æµç»“æŸåè¿”å›å·¥å…·è°ƒç”¨æ‘˜è¦
    
    Args:
        agent: Pydantic AI Agent å®ä¾‹ï¼ˆå·¥å…·å·²ç»‘å®šï¼‰
        user_prompt: ç”¨æˆ·è¾“å…¥çš„æç¤ºè¯
        deps: ä¾èµ–æ³¨å…¥çš„ä¸Šä¸‹æ–‡å¯¹è±¡
        message_history: æ¶ˆæ¯å†å²
        track_tool_calls: æ˜¯å¦åœ¨æµç»“æŸåè¿”å›å·¥å…·è°ƒç”¨æ‘˜è¦
        
    Yields:
        å¢é‡æ–‡æœ¬å†…å®¹æˆ–å·¥å…·è°ƒç”¨æ‘˜è¦ï¼ˆJSON æ ¼å¼ï¼‰
    """
    from pydantic_ai import (
        FinalResultEvent,
        FunctionToolCallEvent,
        FunctionToolResultEvent,
    )
    
    run_kwargs = {"message_history": message_history} if message_history else {}
    
    tool_calls_info = []
    
    # ä½¿ç”¨ iter() è¿­ä»£èŠ‚ç‚¹ï¼ˆåŸºäºå®˜æ–¹ç¤ºä¾‹ï¼‰
    async with agent.iter(user_prompt, deps=deps, **run_kwargs) as run:
        async for node in run:
            # ModelRequestNode - æ¨¡å‹è¯·æ±‚èŠ‚ç‚¹ï¼Œå¯èƒ½åŒ…å«æµå¼æ–‡æœ¬è¾“å‡º
            
            if Agent.is_model_request_node(node):
                async with node.stream(run.ctx) as request_stream:
                    final_result_found = False
                    async for event in request_stream:
                        # æ£€æµ‹åˆ°æœ€ç»ˆç»“æœå¼€å§‹
                        if isinstance(event, FinalResultEvent):
                            final_result_found = True
                            break
                    
                    # å¦‚æœæ£€æµ‹åˆ°æœ€ç»ˆç»“æœï¼Œæµå¼è¾“å‡ºæ–‡æœ¬ï¼ˆå¢é‡æ¨¡å¼ï¼‰
                    if final_result_found:
                        async for output in request_stream.stream_text(delta=True):
                            yield output
            
            # CallToolsNode - å·¥å…·è°ƒç”¨èŠ‚ç‚¹
            elif Agent.is_call_tools_node(node):
                logger.info(node)
                logger.info(f"ğŸ”§ [stream_agent_response] æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨èŠ‚ç‚¹, track_tool_calls={track_tool_calls}")
                if track_tool_calls:
                    async with node.stream(run.ctx) as handle_stream:
                        event_count = 0
                        async for event in handle_stream:
                            event_count += 1
                            logger.info(f"ğŸ” [stream_agent_response] æ”¶åˆ°äº‹ä»¶ #{event_count}, ç±»å‹: {type(event).__name__}")
                            
                            # å·¥å…·è°ƒç”¨äº‹ä»¶
                            if isinstance(event, FunctionToolCallEvent):
                                logger.info(f" [stream_agent_response] ç«‹å³æ¨é€å·¥å…·è°ƒç”¨å¼€å§‹: {event.part.tool_name}")
                                logger.info(f"   å‚æ•°: {event.part.args}")
                                # ç«‹å³é€šçŸ¥å‰ç«¯å·¥å…·è°ƒç”¨å¼€å§‹
                                notification = f"\n\n__TOOL_CALL_START__:{json.dumps({'tool_name': event.part.tool_name, 'args': event.part.args}, ensure_ascii=False)}"
                                yield notification
                                logger.info(f"âœ… [stream_agent_response] å·²æ¨é€é€šçŸ¥åˆ°æµ")
                                
                                tool_calls_info.append({
                                    "tool_name": event.part.tool_name,
                                    "args": event.part.args,
                                    "tool_call_id": event.part.tool_call_id
                                })
                            # å·¥å…·è¿”å›äº‹ä»¶
                            elif isinstance(event, FunctionToolResultEvent):
                                logger.info(f"âœ… [stream_agent_response] å·¥å…·æ‰§è¡Œå®Œæˆ: {event.tool_call_id}")
                                logger.info(f"   è¿”å›ç»“æœ: {event.result.content}")
                                # æŸ¥æ‰¾å¯¹åº”çš„å·¥å…·è°ƒç”¨å¹¶æ·»åŠ è¿”å›ç»“æœ
                                for call_info in tool_calls_info:
                                    if call_info.get("tool_call_id") == event.tool_call_id:
                                        call_info["result"] = event.result.content
                                        logger.info(f" [stream_agent_response] å·²è®°å½•å·¥å…·ç»“æœ")
                                        break
                        
                        logger.info(f"ğŸ [stream_agent_response] å·¥å…·è°ƒç”¨èŠ‚ç‚¹å¤„ç†å®Œæˆï¼Œå…± {event_count} ä¸ªäº‹ä»¶")
                        
                        # å·¥å…·è°ƒç”¨å®Œæˆåï¼Œåœ¨åç»­æ–‡æœ¬å‰åŠ ä¸Šæ¢è¡Œï¼Œé¿å…ç´§æŒ¨åœ¨ä¸€èµ·
                        if event_count > 0:
                            yield "\n\n"
                else:
                    logger.warning(f"âš ï¸ [stream_agent_response] track_tool_calls=Falseï¼Œè·³è¿‡å·¥å…·è°ƒç”¨è¿½è¸ª")
            
            # End - ç»“æŸèŠ‚ç‚¹
            elif Agent.is_end_node(node):
                # è¿è¡Œå®Œæˆ
                pass
    
    # æµç»“æŸåï¼Œè¿”å›å·¥å…·è°ƒç”¨æ‘˜è¦
    if track_tool_calls and tool_calls_info:
        yield f"\n\n__TOOL_SUMMARY__:{json.dumps({'type': 'tools_executed', 'tools': tool_calls_info}, ensure_ascii=False)}"

async def run_llm_agent(
    session: Session,
    llm_config_id: int,
    user_prompt: str,
    output_type: Type[BaseModel],
    system_prompt: Optional[str] = None,
    deps: str = "",
    max_tokens: Optional[int] = None,
    max_retries: int = 3,
    temperature: Optional[float] = None,
    timeout: Optional[float] = None,
    track_stats: bool = True,
) -> BaseModel:
    """
    è¿è¡ŒLLM Agentçš„æ ¸å¿ƒå°è£…ã€‚
    æ”¯æŒæ¸©åº¦/æœ€å¤§tokens/è¶…æ—¶ï¼ˆé€šè¿‡ ModelSettings æ³¨å…¥ï¼‰ã€‚
    """
    # é™é¢é¢„æ£€ï¼ˆæŒ‰ä¼°ç®—çš„è¾“å…¥ tokens + 1 æ¬¡è°ƒç”¨ï¼‰
    if track_stats:
        ok, reason = _precheck_quota(session, llm_config_id, _calc_input_tokens(system_prompt, user_prompt), need_calls=1)
        if not ok:
            raise ValueError(f"LLM é…é¢ä¸è¶³:{reason}")

    agent = _get_agent(
        session,
        llm_config_id,
        output_type,
        system_prompt or '',
        temperature=temperature,
        max_tokens=max_tokens,
        timeout=timeout,
    )
    
    logger.info(f"system_prompt: {system_prompt}")
    logger.info(f"user_prompt: {user_prompt}")
    
    last_exception = None
    for attempt in range(max_retries):
        try:
            response=await run_agent_with_streaming(agent, user_prompt, deps=deps)

            # ç»Ÿè®¡ï¼šè¾“å…¥/è¾“å‡º tokens ä¸è°ƒç”¨æ¬¡æ•°
            if track_stats:
                in_tokens = _calc_input_tokens(system_prompt, user_prompt)
                try:
                    out_text = response if isinstance(response, str) else json.dumps(response, ensure_ascii=False)
                except Exception:
                    out_text = str(response)
                out_tokens = _estimate_tokens(out_text)
                _record_usage(session, llm_config_id, in_tokens, out_tokens, calls=1, aborted=False)
            return response
        except asyncio.CancelledError:
            logger.info("LLM è°ƒç”¨è¢«å–æ¶ˆï¼ˆCancelledErrorï¼‰ï¼Œç«‹å³ä¸­æ­¢ï¼Œä¸å†é‡è¯•ã€‚")
            if track_stats:
                in_tokens = _calc_input_tokens(system_prompt, user_prompt)
                _record_usage(session, llm_config_id, in_tokens, 0, calls=1, aborted=True)
            raise
        except Exception as e:
            last_exception = e
            logger.warning(f"Agent execution failed on attempt {attempt + 1}/{max_retries} for llm_config_id {llm_config_id}: {e}")

    logger.error(f"Agent execution failed after {max_retries} attempts for llm_config_id {llm_config_id}. Last error: {last_exception}")
    raise ValueError(f"è°ƒç”¨LLMæœåŠ¡å¤±è´¥ï¼Œå·²é‡è¯• {max_retries} æ¬¡: {str(last_exception)}")




async def generate_assistant_chat_streaming(
    session: Session,
    request: AssistantChatRequest,
    system_prompt: str,
    tools: list,  #  ç›´æ¥æ¥å—å·¥å…·å‡½æ•°åˆ—è¡¨
    deps,  #  ä¾èµ–ä¸Šä¸‹æ–‡ï¼ˆAssistantDeps å®ä¾‹ï¼‰
    track_stats: bool = True
) -> AsyncGenerator[str, None]:
    """
    çµæ„ŸåŠ©æ‰‹ä¸“ç”¨æµå¼å¯¹è¯ç”Ÿæˆã€‚
    
    å‚æ•°ï¼š
    - request: AssistantChatRequestï¼ˆåŒ…å«å¯¹è¯å†å²ã€å¡ç‰‡ä¸Šä¸‹æ–‡ç­‰ï¼‰
    - system_prompt: ç³»ç»Ÿæç¤ºè¯
    - tools: å·¥å…·å‡½æ•°åˆ—è¡¨ï¼ˆç›´æ¥ä¼ å‡½æ•°ï¼Œç¬¦åˆ Pydantic AI æ ‡å‡†ç”¨æ³•ï¼‰
    - deps: å·¥å…·ä¾èµ–ä¸Šä¸‹æ–‡ï¼ˆAssistantDeps å®ä¾‹ï¼‰
    - track_stats: æ˜¯å¦ç»Ÿè®¡ token ä½¿ç”¨
    """
    
    # ç›´æ¥ä½¿ç”¨å‰ç«¯å‘é€çš„ä¸Šä¸‹æ–‡å’Œç”¨æˆ·è¾“å…¥
    parts = []
    
    # 1. é¡¹ç›®ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆåŒ…å«ç»“æ„æ ‘ã€ç»Ÿè®¡ã€æ“ä½œå†å²ç­‰ï¼‰
    if request.context_info:
        parts.append(request.context_info)
    
    # 2. ç”¨æˆ·å½“å‰è¾“å…¥
    if request.user_prompt:
        parts.append(f"\n**ç”¨æˆ·è¯´**ï¼š{request.user_prompt}")
    
    final_user_prompt = "\n\n".join(parts) if parts else "ï¼ˆç”¨æˆ·æœªè¾“å…¥æ–‡å­—ï¼Œå¯èƒ½æ˜¯æƒ³æŸ¥çœ‹é¡¹ç›®ä¿¡æ¯æˆ–éœ€è¦å¸®åŠ©ï¼‰"
    
    logger.info(f"çµæ„ŸåŠ©æ‰‹ system_prompt: {system_prompt}...")
    logger.info(f"çµæ„ŸåŠ©æ‰‹ final_user_prompt: {final_user_prompt}...")
    
    # é™é¢é¢„æ£€
    if track_stats:
        ok, reason = _precheck_quota(session, request.llm_config_id, _calc_input_tokens(system_prompt, final_user_prompt), need_calls=1)
        if not ok:
            raise ValueError(f"LLM é…é¢ä¸è¶³:{reason}")
    
    # åˆ›å»º Agentï¼ˆå¸¦å·¥å…·ï¼‰
    from app.services.assistant_tools.pydantic_ai_tools import AssistantDeps
    
    # ç›´æ¥åœ¨åˆ›å»ºæ—¶ä¼ å…¥å·¥å…·åˆ—è¡¨ï¼ˆPydantic AI æ¨èæ–¹å¼ï¼‰
    agent = _get_agent(
        session=session,
        llm_config_id=request.llm_config_id,
        output_type=None,  # å…è®¸å·¥å…·è°ƒç”¨
        system_prompt=system_prompt,
        temperature=request.temperature or 0.7,
        max_tokens=request.max_tokens or 4096,
        timeout=request.timeout or 60,
        deps_type=AssistantDeps,
        tools=tools  # ç›´æ¥ä¼ å…¥å·¥å…·å‡½æ•°åˆ—è¡¨
    )
    
    # æµå¼ç”Ÿæˆå“åº”
    accumulated = ""
    
    try:
        async for chunk in stream_agent_response(
            agent,
            final_user_prompt,
            deps=deps,  # ä¼ å…¥ä¾èµ–ä¸Šä¸‹æ–‡
            track_tool_calls=True
        ):
            accumulated += chunk
            yield chunk
        
    except asyncio.CancelledError:
        if track_stats:
            in_tokens = _calc_input_tokens(system_prompt, final_user_prompt)
            out_tokens = _estimate_tokens(accumulated)
            _record_usage(session, request.llm_config_id, in_tokens, out_tokens, calls=1, aborted=True)
        return
    except Exception as e:
        logger.error(f"çµæ„ŸåŠ©æ‰‹ç”Ÿæˆå¤±è´¥: {e}")
        # å³ä½¿å¤±è´¥ä¹Ÿè¦å‘é€é”™è¯¯æ‘˜è¦ï¼Œè®©å‰ç«¯æ¸…é™¤"æ­£åœ¨è°ƒç”¨å·¥å…·"çŠ¶æ€
        yield f"\n\n__ERROR__:{json.dumps({'type': 'error', 'message': str(e)}, ensure_ascii=False)}"
        raise
    
    # ç»Ÿè®¡
    if track_stats:
        try:
            in_tokens = _calc_input_tokens(system_prompt, final_user_prompt)
            out_tokens = _estimate_tokens(accumulated)
            _record_usage(session, request.llm_config_id, in_tokens, out_tokens, calls=1, aborted=False)
        except Exception as stat_e:
            logger.warning(f"è®°å½•çµæ„ŸåŠ©æ‰‹ç»Ÿè®¡å¤±è´¥: {stat_e}")


async def generate_continuation_streaming(session: Session, request: ContinuationRequest, system_prompt: str, track_stats: bool = True) -> AsyncGenerator[str, None]:
    """ä»¥æµå¼æ–¹å¼ç”Ÿæˆç»­å†™å†…å®¹ã€‚system_prompt ç”±å¤–éƒ¨æ˜¾å¼ä¼ å…¥ã€‚"""
    # ç»„è£…ç”¨æˆ·æ¶ˆæ¯
    user_prompt_parts = []
    
    # 1. æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå¼•ç”¨ä¸Šä¸‹æ–‡ + äº‹å®å­å›¾ï¼‰
    context_info = (getattr(request, 'context_info', None) or '').strip()
    if context_info:
        # æ£€æµ‹ context_info æ˜¯å¦å·²åŒ…å«ç»“æ„åŒ–æ ‡è®°ï¼ˆå¦‚ã€å¼•ç”¨ä¸Šä¸‹æ–‡ã€‘ã€ã€ä¸Šæ–‡ã€‘ç­‰ï¼‰
        has_structured_marks = any(mark in context_info for mark in ['ã€å¼•ç”¨ä¸Šä¸‹æ–‡ã€‘', 'ã€ä¸Šæ–‡ã€‘', 'ã€éœ€è¦æ¶¦è‰²', 'ã€éœ€è¦æ‰©å†™'])
        
        if has_structured_marks:
            # å·²ç»æ˜¯ç»“æ„åŒ–çš„ä¸Šä¸‹æ–‡ï¼Œç›´æ¥ä½¿ç”¨ï¼Œä¸å†é¢å¤–åŒ…è£¹
            user_prompt_parts.append(context_info)
        else:
            # æœªç»“æ„åŒ–çš„ä¸Šä¸‹æ–‡ï¼ˆè€æ ¼å¼ï¼‰ï¼Œæ·»åŠ æ ‡è®°
            user_prompt_parts.append(f"ã€å‚è€ƒä¸Šä¸‹æ–‡ã€‘\n{context_info}")
    
    # 2. æ·»åŠ å·²æœ‰ç« èŠ‚å†…å®¹ï¼ˆä»…å½“ previous_content éç©ºæ—¶ï¼‰
    previous_content = (request.previous_content or '').strip()
    if previous_content:
        user_prompt_parts.append(f"ã€å·²æœ‰ç« èŠ‚å†…å®¹ã€‘\n{previous_content}")
        
        # æ·»åŠ å­—æ•°ç»Ÿè®¡ä¿¡æ¯
        existing_word_count = getattr(request, 'existing_word_count', None)
        if existing_word_count is not None:
            user_prompt_parts.append(f"ï¼ˆå·²æœ‰å†…å®¹å­—æ•°ï¼š{existing_word_count} å­—ï¼‰")
        
        # ç»­å†™æŒ‡ä»¤
        if getattr(request, 'append_continuous_novel_directive', True):
            user_prompt_parts.append("ã€æŒ‡ä»¤ã€‘è¯·æ¥ç€ä¸Šè¿°å†…å®¹ç»§ç»­å†™ä½œï¼Œä¿æŒæ–‡é£å’Œå‰§æƒ…è¿è´¯ã€‚ç›´æ¥è¾“å‡ºå°è¯´æ­£æ–‡ã€‚")
    else:
        # æ–°å†™æ¨¡å¼æˆ–æ¶¦è‰²/æ‰©å†™æ¨¡å¼ï¼ˆprevious_content ä¸ºç©ºï¼‰
        # åªåœ¨éœ€è¦æ—¶æ·»åŠ æŒ‡ä»¤
        if getattr(request, 'append_continuous_novel_directive', True):
            # å¦‚æœ context_info ä¸­æœ‰ç»­å†™ç›¸å…³æ ‡è®°ï¼Œè¯´æ˜æ˜¯ç»­å†™åœºæ™¯
            if context_info and 'ã€å·²æœ‰ç« èŠ‚å†…å®¹ã€‘' in context_info:
                user_prompt_parts.append("ã€æŒ‡ä»¤ã€‘è¯·æ¥ç€ä¸Šè¿°å†…å®¹ç»§ç»­å†™ä½œï¼Œä¿æŒæ–‡é£å’Œå‰§æƒ…è¿è´¯ã€‚ç›´æ¥è¾“å‡ºå°è¯´æ­£æ–‡ã€‚")
            else:
                user_prompt_parts.append("ã€æŒ‡ä»¤ã€‘è¯·å¼€å§‹åˆ›ä½œæ–°ç« èŠ‚ã€‚ç›´æ¥è¾“å‡ºå°è¯´æ­£æ–‡ã€‚")
    
    user_prompt = "\n\n".join(user_prompt_parts)
    
    # é™é¢é¢„æ£€
    if track_stats:
        ok, reason = _precheck_quota(session, request.llm_config_id, _calc_input_tokens(system_prompt, user_prompt), need_calls=1)
        if not ok:
            raise ValueError(f"LLM é…é¢ä¸è¶³:{reason}")

    agent = _get_agent(
        session,
        request.llm_config_id,
        output_type=BaseModel,
        system_prompt=system_prompt,
        temperature=request.temperature,
        max_tokens=request.max_tokens,
        timeout=request.timeout,
    )

    try:
        logger.debug(f"æ­£åœ¨ä»¥æµå¼æ¨¡å¼è¿è¡Œ agent")
        async with agent.run_stream(user_prompt) as result:
            accumulated: str = ""
            # ç»Ÿè®¡ç”¨ï¼šç´¯ç§¯è¾“å‡ºå­—ç¬¦æ•°
            out_chars: int = 0
            async for text_chunk in result.stream():
                try:
                    chunk = str(text_chunk)
                except Exception:
                    chunk = ""
                if not chunk:
                    continue
                # è‹¥è¿”å›çš„æ˜¯ç´¯è®¡æ–‡æœ¬ï¼Œåªå‘é€æ–°å¢å·®é‡
                if accumulated and chunk.startswith(accumulated):
                    delta = chunk[len(accumulated):]
                else:
                    # å›é€€ï¼šæ— æ³•åˆ¤æ–­å‰ç¼€æ—¶ï¼Œç›´æ¥æŠŠæœ¬æ¬¡ chunk ä½œä¸ºå¢é‡
                    delta = chunk
                if delta:
                    out_chars += len(delta)
                    yield delta
                if len(chunk) > len(accumulated):
                    accumulated = chunk
    except asyncio.CancelledError:
        logger.info("æµå¼ LLM è°ƒç”¨è¢«å–æ¶ˆï¼ˆCancelledErrorï¼‰ï¼Œåœæ­¢æ¨é€ã€‚")
        if track_stats:
            in_tokens = _calc_input_tokens(system_prompt, user_prompt)
            out_tokens = _estimate_tokens(accumulated)
            _record_usage(session, request.llm_config_id, in_tokens, out_tokens, calls=1, aborted=True)
        return
    except Exception as e:
        logger.error(f"æµå¼ LLM è°ƒç”¨å¤±è´¥: {e}")
        raise
    # æ­£å¸¸ç»“æŸåç»Ÿè®¡
    try:
        if track_stats:
            in_tokens = _calc_input_tokens(system_prompt, user_prompt)
            out_tokens = _estimate_tokens(accumulated)
            _record_usage(session, request.llm_config_id, in_tokens, out_tokens, calls=1, aborted=False)
    except Exception as stat_e:
        logger.warning(f"è®°å½• LLM æµå¼ç»Ÿè®¡å¤±è´¥: {stat_e}")


def create_validator(model_type: Type[BaseModel]) -> Callable[[Any, Any], Awaitable[BaseModel]]:
    '''åˆ›å»ºé€šç”¨çš„ç»“æœéªŒè¯å™¨'''

    async def validate_result(
        ctx: Any,
        result: Response,
    ) -> Response:
        """
        é€šç”¨ç»“æœéªŒè¯å‡½æ•°
        """
        try:
            if model_type is BaseModel or model_type is str: 
                return result
        except Exception:
            return result

        # å°è¯•è§£æä¸ºç›®æ ‡æ¨¡å‹
        parsed: BaseModel
        if isinstance(result, model_type):
            parsed = result
        else:
            try:
                print(f"result: {result}")
                parsed = model_type.model_validate_json(repair_json(result))
            except ValidationError as e:
                err_msg = e.json(include_url=False)
                print(f"Invalid {err_msg}")
                raise ModelRetry(f"Invalid  {err_msg}")
            except Exception as e:
                print("Exception:", e)
                raise ModelRetry(f'Invalid {e}') from e

        # === é’ˆå¯¹ StageLine/ChapterOutline/Chapter çš„å®ä½“å­˜åœ¨æ€§æ ¡éªŒ ===
        try:
            # è§£æ deps ä¸­çš„å®ä½“åç§°é›†åˆ
            all_names: set[str] = set()
            try:
                deps_obj = json.loads(getattr(ctx, 'deps', '') or '{}')
                for nm in (deps_obj.get('all_entity_names') or []):
                    if isinstance(nm, str) and nm.strip():
                        all_names.add(nm.strip())
            except Exception:
                all_names = set()
                
            

            def _check_entity_list(obj: Any) -> list[str]:
                bad: list[str] = []
                try:
                    items = getattr(obj, 'entity_list', None)
                    if isinstance(items, list):
                        for it in items:
                            nm = getattr(it, 'name', None)
                            if isinstance(nm, str) and nm.strip():
                                if all_names and nm.strip() not in all_names:
                                    bad.append(nm.strip())
                except Exception:
                    pass
                return bad

            invalid: list[str] = []
            if isinstance(parsed, (StageLine, ChapterOutline, Chapter)):
                logger.info(f"å¼€å§‹æ ¡éªŒå®ä½“,all_names: {all_names}")
                invalid = _check_entity_list(parsed)
       

            if invalid:
                raise ModelRetry(f"å®ä½“ä¸å­˜åœ¨: {', '.join(sorted(set(invalid)))}ï¼Œè¯·ä»…ä»æä¾›çš„å®ä½“åˆ—è¡¨ä¸­é€‰æ‹©")
        except ModelRetry:
            raise
        except Exception:
            # æ ¡éªŒè¿‡ç¨‹ä¸­ä¸åº”é˜»å¡ä¸»æµç¨‹ï¼Œå¦‚è§£æå¤±è´¥åˆ™å¿½ç•¥
            pass

        return parsed 

    return validate_result